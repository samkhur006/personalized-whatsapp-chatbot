# Llama 3.2 1B Training Configuration

model:
  name: "llama32_1b"
  base_model: "meta-llama/Llama-3.2-1B"  # Can be HF model or checkpoint path
  # base_model: "./checkpoints/my_previous_training"  # Example: use previous checkpoint
  load_from_checkpoint: false  # Set to true if base_model is a checkpoint path
  # Model architecture (Llama 3.2 1B)
  num_layers: 16
  hidden_size: 2048
  ffn_hidden_size: 8192
  num_attention_heads: 32
  num_query_groups: 8
  kv_channels: 64
  position_embedding_type: "rope"
  rotary_base: 500000
  rotary_percent: 1.0
  attention_dropout: 0.0
  hidden_dropout: 0.0
  swiglu: true
  init_method_std: 0.0134
  attention_backend: "fused"
  apply_layernorm_1p: true
  untie_embeddings_and_output_weights: true
  disable_bias_linear: true
  normalization: "RMSNorm"
  
training:
  # Batch sizes (optimized for single GPU)
  micro_batch_size: 2
  global_batch_size: 8
  
  # Sequence length
  seq_length: 4096
  max_position_embeddings: 4096
  
  # Learning rate (slightly higher for smaller model)
  learning_rate: 1e-4
  min_learning_rate: 1e-5
  warmup_steps: 200
  
  # Training steps
  train_steps: 5000
  eval_interval: 100
  save_interval: 500
  
  # Optimization
  weight_decay: 0.01
  grad_clip: 1.0
  
  # Precision
  use_bf16: true
  use_fp8: false

parallelism:
  tensor_parallel_size: 1
  pipeline_parallel_size: 1
  context_parallel_size: 1
  
data:
  data_prefix: "MOCK"
  tokenizer_model: "meta-llama/Llama-3.2-1B"
  vocab_size: 128256

logging:
  log_interval: 10
  tensorboard_dir: "./tensorboard_logs"
  checkpoint_dir: "./checkpoints"
